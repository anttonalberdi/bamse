#!/usr/bin/env python

import os
import sys
import glob
from glob import glob
import shutil

####################
# Check if conda environment bamse-env is active #
####################

syspath = sys.prefix

if not "bamse-env" in syspath:
    print("#####################")
    print("#### BAMSE v1.0 ##### ERROR!")
    print("#####################")
    print("\nThe conda environment bamse-env is not activated\nType the following code before running bamse:\n\tconda activate bamse-env\n")
    sys.exit(0)

import argparse
import subprocess
import ruamel.yaml
import pathlib
import re
import time
from shutil import which
import Bio
from Bio import SeqIO
import numpy as np
import statistics

####################
# Argument parsing #
####################
# add overwrite option -w

parser = argparse.ArgumentParser(description='Runs BAMSE pipeline.')
parser.add_argument('-i', help="Data information file", dest="input", required=True)
parser.add_argument('-d', help="Working directory of the project", dest="workdir", required=True)
parser.add_argument('-f', help="Forward primer sequence", dest="primF", required=False)
parser.add_argument('-r', help="Reverse primer sequence", dest="primR", required=False)
parser.add_argument('-a', help="Expected average amplicon length", dest="ampliconlength", required=True)
parser.add_argument('-x', help="Absolute path to the taxonomy database", dest="tax", required=True)
parser.add_argument('-t', help="Number of threads", dest="threads", required=False)
parser.add_argument('-e', help="Maximum number of expected errors per read", dest="mee", required=False)
parser.add_argument('-o', help="Minimum overlap between reads for merging", dest="overlap", required=False)
parser.add_argument('-s', help="Trimming length of the forward reads", dest="trimfor", required=False)
parser.add_argument('-z', help="Trimming length of the reverse reads", dest="trimrev", required=False)
parser.add_argument('-n', help="Maximum number of reads to be analysed to find optimal trimming parameters", dest="maxreads", required=False)
parser.add_argument('-y', help="Taxonomy filtering threshold", dest="taxfilter", required=False)
parser.add_argument('-m', help="Minimum fold to consider parent ASVs for chimera detection", dest="fold", required=False)
parser.add_argument('-u', help="Whether to run LULU clustering", dest="lulu", required=False ,action='store_true')
parser.add_argument('-p', help="Absolute path to the parameters file that BAMSE will create", dest="param", required=False)
parser.add_argument('-l', help="Absolute path to the log file that BAMSE will create", dest="log", required=False)
parser.add_argument('-w', help="Overwrite contents in the working directory of the project", action='store_true', dest="overwrite", required=False)
args = parser.parse_args()

# Translate arguments
in_f=args.input
path=args.workdir
ampliconlength=args.ampliconlength
tax=args.tax
fold=args.fold
overwrite=args.overwrite
luluarg=args.lulu

# Retrieve current directory
file = os.path.dirname(sys.argv[0])
curr_dir = os.path.abspath(file)

# Create workign directory if does not exist
if not os.path.exists(path):
    os.makedirs(path)

#Remove last / to the working directory (if necessary)
path = re.sub('/$','',path)

#Define primers
if not (args.primF):
    primF='noprimer'
else:
    primF=args.primF

if not (args.primR):
    primR='noprimer'
else:
    primR=args.primR

# Define param file
if not (args.param):
    param=os.path.join(os.path.abspath(path),"bamse.yaml")
else:
    param=args.param

# Define log file
if not (args.log):
    log=os.path.join(path,"bamse.log")
else:
    log=args.log

# Define number of cores
if not (args.log):
    cores=4
else:
    cores=args.threads

# Define trimming parameters
if not (args.trimfor):
    trimfor='auto'
else:
    trimfor=args.trimfor

if not (args.trimrev):
    trimrev='auto'
else:
    trimrev=args.trimrev

#Define maximum number of reads
if not (args.maxreads):
    maxreads=10000
else:
    maxreads=args.maxreads

# Define quality filtering value
if not (args.taxfilter):
    taxfilter='kingdom'
else:
    taxfilter=args.taxfilter

# Define quality filtering value
if not (args.mee):
    mee=2
else:
    mee=args.mee

# Define overlap value
if not (args.overlap):
    overlap=20
else:
    overlap=args.overlap

# Define quality filtering value
if not (args.fold):
    fold=1
else:
    fold=args.fold

#Remove param file if exists
if os.path.exists(param):
    os.remove(param)

curr_dir=os.path.dirname(sys.argv[0])
bamsepath=os.path.abspath(curr_dir)

# Remove contents in the directory
if overwrite == True:
    if os.path.isdir(path+"/0-Data"):
        shutil.rmtree(path+"/0-Data")
    if os.path.isdir(path+"/0-Stats"):
        shutil.rmtree(path+"/0-Stats")
    if os.path.isdir(path+"/1-Primertrimmed"):
        shutil.rmtree(path+"/1-Primertrimmed")
    if os.path.isdir(path+"/2-Filtered"):
        shutil.rmtree(path+"/2-Filtered")
    if os.path.isdir(path+"/3-DADA2"):
        shutil.rmtree(path+"/3-DADA2")
    if os.path.isdir(path+"/4-Taxonomyfilter"):
        shutil.rmtree(path+"/4-Taxonomyfilter")
    if os.path.isdir(path+"/6-Phylogeny"):
        shutil.rmtree(path+"/6-Phylogeny")
    if os.path.isdir(path+"/7-Binning"):
        shutil.rmtree(path+"/7-Binning")
    if os.path.isfile(path+"/ASV_counts.binned.csv"):
        os.remove(path+"/ASV_counts.binned.csv")
    if os.path.isfile(path+"/ASV_counts.csv"):
        os.remove(path+"/ASV_counts.csv")
    if os.path.isfile(path+"/ASV_taxa.txt"):
        os.remove(path+"/ASV_taxa.txt")
    if os.path.isfile(path+"/ASVs.binned.fasta"):
        os.remove(path+"/ASVs.binned.fasta")
    if os.path.isfile(path+"/ASVs.binned.tre"):
        os.remove(path+"/ASVs.binned.tre")
    if os.path.isfile(path+"/ASVs.fasta"):
        os.remove(path+"/ASVs.fasta")
    if os.path.isfile(path+"/ASVs.tre"):
        os.remove(path+"/ASVs.tre")
    if os.path.isfile(path+"/bamse.log"):
        os.remove(path+"/bamse.log")
    if os.path.isfile(path+"/bamse.yaml"):
        os.remove(path+"/bamse.yaml")

###################
# Create log file #
###################

logfile=open(log,"w+")
logfile.write("#####################\n")
logfile.write("#### BAMSE v1.0 #####\n")
logfile.write("#####################\n")
logfile.write("BAMSE is starting at:\n")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\t{0}\n".format(current_time))
logfile.close()

#Print on screen
print("\n#######################################################################\n")
print("     (^).-.(^)      ██████╗  █████╗ ███╗   ███╗███████╗███████╗")
print("      / o_o \       ██╔══██╗██╔══██╗████╗ ████║██╔════╝██╔════╝")
print("    __\(`-´)/__     ██████╔╝███████║██╔████╔██║███████╗█████╗ ")
print("   (__ /'-'\ __)    ██╔══██╗██╔══██║██║╚██╔╝██║╚════██║██╔══╝ ")
print("      || o ||       ██████╔╝██║  ██║██║ ╚═╝ ██║███████║███████╗")
print("     _/ '_' \_      ╚═════╝ ╚═╝  ╚═╝╚═╝     ╚═╝╚══════╝╚══════╝")
print("     `-´   `-´      Bacterial AMplicon SEquence processing pipeline")
print("    Version: 1.0    By @Antton Alberdi & @Ostaizka Aizpurua\n")
print("#######################################################################\n")
print("BAMSE is starting at:")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
print("\t{0}".format(current_time))

#############################################
# Append information to the parameters file #
#############################################

#Append information to the parameters file
f = open(str(param), "a")
f.write("#BAMSE core paths\n")
f.write("bamsepath:\n "+str(curr_dir)+"\n")
f.write("projectpath:\n "+str(path)+"\n")
f.write("parampath:\n "+str(param)+"\n")
f.write("logpath:\n "+str(log)+"\n")
f.write("taxonomy:\n "+str(tax)+"\n")
f.write("\n#Primers\n")
f.write("primer1:\n "+str(primF)+"\n")
f.write("primer2:\n "+str(primR)+"\n")
f.write("\n#Trimming and filtering\n")
f.write("ampliconlength:\n "+str(ampliconlength)+"\n")
f.write("mee:\n "+str(mee)+"\n")
f.write("overlap:\n "+str(overlap)+"\n")
f.write("maxreads:\n "+str(maxreads)+"\n")
f.write("\n#Chimera filtering\n")
f.write("fold:\n "+str(fold)+"\n")
f.write("\n#Taxonomy filtering\n")
f.write("taxfilter:\n "+str(taxfilter)+"\n")
f.close()

#Append information to the log file
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\nBAMSE is running with the following parameters:\n")
logfile.write("#Paths\n")
logfile.write("\tbamsepath: "+str(curr_dir)+"\n")
logfile.write("\tprojectpath: "+str(path)+"\n")
logfile.write("\tparampath: "+str(param)+"\n")
logfile.write("\tlogpath: "+str(log)+"\n")
logfile.write("\ttaxonomy: "+str(tax)+"\n")
logfile.write("#Primers\n")
logfile.write("\tprimer1: "+str(primF)+"\n")
logfile.write("\tprimer2: "+str(primR)+"\n")
logfile.write("#Parameters\n")
logfile.write("\tampliconlength: "+str(ampliconlength)+"\n")
logfile.write("\tmee: "+str(mee)+"\n")
logfile.write("\toverlap: "+str(overlap)+"\n")
logfile.write("\tmaxreads: "+str(maxreads)+"\n")
logfile.write("\tfold: "+str(fold)+"\n")
logfile.write("taxfilter:\n "+str(taxfilter)+"\n")
logfile.close()

#Print information on screen
print("\nBAMSE is running with the following parameters:\n")
print("PATHS")
print("\tBamse path: "+str(curr_dir))
print("\tProject path: "+str(path))
print("\tParameters file path: "+str(param))
print("\tLog file path: "+str(log))
print("\tTaxonomy database path: "+str(tax)+"\n")
print("PRIMERS")
print("\tForward primer: "+str(primF))
print("\tReverse primer: "+str(primR)+"\n")
print("PARAMETERS")
print("\tAmplicon length: "+str(ampliconlength))
print("\tMaximum Expected Error: "+str(mee))
print("\tMinimum read overlap: "+str(overlap))
print("\tForward read trimming length: "+str(trimfor))
print("\tReverse read trimming length: "+str(trimrev))
print("\tMaximum number of reads for parameter optimisation: "+str(maxreads))
print("\tChimera-assigning fold: "+str(fold))
print("\tTaxonomic filtering level: "+str(taxfilter)+"\n")
print("If you want to learn more about these parameters visit:")
print("\thttps://github.com/anttonalberdi/bamse\n")
print("##########################################")

######################
# Check dependencies #
######################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Checking dependencies \r\n".format(current_time))
print("\n{0} | CHECKING DEPENDENCIES".format(current_time))

def is_tool(name):
    return which(name) is not None

if not is_tool('perl'):
    logfile.write("\tERROR! Perl is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('snakemake'):
    logfile.write("\tERROR! Snakemake is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('R'):
    logfile.write("\tERROR! R is not installed or loaded. \n")
    sys.exit(0)

else:
    logfile.write("\tAll dependencies are properly installed. \n")
    print("\tAll dependencies are properly installed.")
    logfile.close()


#########################################
# Check if input information is correct #
#########################################

if not os.path.isfile(in_f):
    print("ERROR! The input data file does not exist.")
    logfile.write("\tERROR! The input data file does not exist. \n")
    sys.exit(0)

if not os.path.isfile(tax):
    print("ERROR! The taxonomy database path is incorrect.")
    logfile.write("\tERROR! The taxonomy database path is incorrect. \n")
    sys.exit(0)

if not (taxfilter == 'kingdom') and (taxfilter == 'phylum') and (taxfilter == 'class') and (taxfilter == 'order') and (taxfilter == 'family') and (taxfilter == 'genus'):
    logfile=open(log,"a+")
    logfile.write("\tERROR! The assigned taxonomy filter " + taxfilter + " is incorrect.\n")
    logfile.write("\tThe allowed values include: 'division', 'phylum', 'class', 'order', 'family' and 'genus'.\n")
    logfile.close()
    print("\tERROR! The assigned taxonomy parameter " + taxfilter + " is incorrect")
    print("\tThe allowed values include: 'division', 'phylum', 'class', 'order', 'family' and 'genus.")
    sys.exit(0)

###############################
# Prepare working directories #
###############################

# Set input directory
dir0 = os.path.join(path,"0-Data")
#dir1 = os.path.join(path,"1-Trimmed")
#dir2 = os.path.join(path,"2-Filtered")

## If input directory does not exist, make it
if not os.path.exists(dir0):
    os.makedirs(dir0)

## If stats directory does not exist, make it
statsdir = os.path.join(path,"0-Stats")
if not os.path.exists(statsdir):
    os.makedirs(statsdir)

#################
# Transfer data #
#################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | TRANSFERRING DATA TO THE WORKING DIRECTORY \r\n".format(current_time))
logfile.close()
print("\n{0} | TRANSFERRING DATA TO THE WORKING DIRECTORY AND GENERATING BASIC STATISTICS".format(current_time))


####
# Add error if input data contains duplicated sequences
####

# Read input data file
inputfile = open(in_f, "r")

#Declare empty output file list
primlist = []
csvlist = []
filtlist = []
dadalist = []
namelist = []

## Read input data row by row
for line in inputfile:
    ### Skip line if starts with # (comment line)
    if not (line.startswith('#')):
        ###Skip line if it's empty
        if not len(line.strip()) == 0 :
            #Define variables
            line=line.replace('\n', '')
            linelist = line.split(',') # Create a list of each line

            #Test whether the list contains 4 elements
            if len(linelist) != 4:
                logfile=open(log,"a+")
                logfile.write("\tThe format of the data input file is incorrect.\n")
                print("\tERROR! The format of the data input file is incorrect.")
                print("\tEnsure the file is comma-separated and contains")
                print("\tfour columns: SAMPLE, POOL, FORWARD, REVERSE. e.g.:")
                print("\tSAMPLE1,POOL1,Sample1_1.fq.gz,Sample1_2.fq.gz")
                print("\tSAMPLE2,POOL1,Sample2_1.fq.gz,Sample2_2.fq.gz\n")
                logfile.close()
                sys.exit(0)

            #Assign variables
            name=linelist[0]
            run=linelist[1]
            in_for=linelist[2]
            in_rev=linelist[3]

            #Create run folder if it does not exist
            runpath=path+'/0-Data/'+run
            if not os.path.exists(runpath):
                os.makedirs(runpath)

            #Create list of final files for workflow DADA2 A
            dada_run = path+'/3-DADA2/'+run+'.rds'
            dadalist.append(dada_run)

            # Transfer, rename and decompress data

            #Declare output files
            out1=runpath+'/'+name+'_1.fastq'
            out2=runpath+'/'+name+'_2.fastq'
            #Check if the file is already in the working directory

            if os.path.isfile(out1) and os.path.isfile(out2):
                print("\t"+name+"\tThe files are already in the project directory.")
                #Create lists of output files
                prim_for = path+'/1-Primertrimmed/'+run+'/'+name+'_1.fastq'
                prim_rev = path+'/1-Primertrimmed/'+run+'/'+name+'_2.fastq'
                out_csv = path+'/2-Filtered/'+run+'/'+name+'.csv'
                filt_for = path+'/2-Filtered/'+run+'/'+name+'_1.fastq'
                filt_rev = path+'/2-Filtered/'+run+'/'+name+'_2.fastq'

                namelist.append(name)
                primlist.append(prim_for)
                primlist.append(prim_rev)
                csvlist.append(out_csv)
                filtlist.append(filt_for)
                filtlist.append(filt_rev)
            else:
                #If the forward file is not in the working directory, transfer it
                if os.path.isfile(in_for):
                    if in_for.endswith('.gz'):
                        read1Cmd = 'gunzip -c '+in_for+' > '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                    else:
                        read1Cmd = 'cat '+in_for+' > '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                    if os.stat(in_for).st_size == 0:
                        logfile=open(log,"a+")
                        logfile.write("\tThe file " + in_for + " is empty.\n")
                        logfile.close()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_for + " does not exist.\n")
                    logfile.close()

                #If the reverse file is not in the working directory, transfer it
                if os.path.isfile(in_rev):
                    if in_for.endswith('.gz'):
                        read2Cmd = 'gunzip -c '+in_rev+' > '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                    else:
                        read2Cmd = 'cat '+in_rev+' > '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                    if os.stat(in_for).st_size > 0:
                        logfile=open(log,"a+")
                        logfile.write("\tThe file " + in_rev + " is empty.\n")
                        logfile.close()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_rev + " does not exist.\n")
                    logfile.close()

                if os.path.isfile(out1) and os.path.isfile(out2) and (os.stat(out1).st_size > 0) and (os.stat(out2).st_size > 0):
                    #Calculate read number
                    readnum=sum(1 for line in open(out1)) / 4

                    if readnum > 0:
                        #Calculate read lengths
                        fastq_parser1 = SeqIO.parse(out1, "fastq")
                        fastq_parser2 = SeqIO.parse(out2, "fastq")

                        #Calculate read length
                        read1lenlist=[]
                        read2lenlist=[]
                        iter=0
                        for read1, read2 in zip(fastq_parser1, fastq_parser2):
                            if iter < 5000:
                            	read1len=len(read1)
                            	read2len=len(read2)
                            	read1lenlist.append(read1len)
                            	read2lenlist.append(read2len)
                            	iter += 1
                            else:
                                break

                        #Number of reads and read length
                        read1len=int(statistics.mean(read1lenlist))
                        read2len=int(statistics.mean(read2lenlist))
                        statfile=path+'/0-Stats/'+name+'.txt'
                        with open(statfile,'w') as f:
                            f.write('Number of initial reads:\t'+str(int(readnum))+'\n')
                            f.write('Average forward read length:\t'+str(int(read1len))+'\n')
                            f.write('Average reverse read length:\t'+str(int(read2len))+'\n')
                        print("\t"+name+"\t"+str(int(readnum))+" reads of average length "+str(int(read1len))+" (F) and "+str(int(read2len))+" (R) nucleotides.")
                    else:
                        with open(statfile,'w') as f:
                            f.write('Number of initial reads:\t'+str(int(readnum))+'\n')

                    #Create lists of output files
                    prim_for = path+'/1-Primertrimmed/'+run+'/'+name+'_1.fastq'
                    prim_rev = path+'/1-Primertrimmed/'+run+'/'+name+'_2.fastq'
                    out_csv = path+'/2-Filtered/'+run+'/'+name+'.csv'
                    filt_for = path+'/2-Filtered/'+run+'/'+name+'_1.fastq'
                    filt_rev = path+'/2-Filtered/'+run+'/'+name+'_2.fastq'

                    namelist.append(name)
                    primlist.append(prim_for)
                    primlist.append(prim_rev)
                    csvlist.append(out_csv)
                    filtlist.append(filt_for)
                    filtlist.append(filt_rev)
                else:
                    print("\t"+name+"\tERROR! The files are not available or are empty.")

print(filtlist)
#####################################################
################## Data assessment ##################
#####################################################

print("\n{0} | PERFORMING DATA AND PARAMETER ASSESSMENT".format(current_time))

statfiles = glob(path+'/0-Stats/*.txt')
statsdata = [np.loadtxt(f, delimiter='\t', usecols=1) for f in statfiles]

numbersamples=len(statsdata)
avgreads=np.average(statsdata,axis=0)[0]
avgreadlen1=np.average(statsdata,axis=0)[1]
avgreadlen2=np.average(statsdata,axis=0)[2]
avgreads=avgreadlen1+avgreadlen2

print("\tNumber of samples: " + str(numbersamples))
print("\tAverage number of reads: " + str(avgreads))
print("\tAverage length of forward reads: " + str(avgreadlen1))
print("\tAverage length of reverse reads: " + str(avgreadlen2))

#primF='CTANGGGNNGCANCAG'
#primR='GACTACNNGGGTATCTAAT'
#overlap=20

#Subtract primer length if these are provided
if (primF != 'noprimer') & (primR != 'noprimer'):
    avgreads = avgreads - len(primF) - len(primR)
    print("\tLength of forward primer: " + str(len(primF)))
    print("\tLength of reverse primer: " +  str(len(primR)))

#Calculate whether the length of the reads + overlap are sufficient for covering the amplicon length

print("\n\tBAMSE will now calculate the excess of nucleotides after subtracting the amplicon length and the")
print("\tminimum number of overlapping nucleotides to assess whether the analysis can continue:")
print("\t\tExcess = (READ1+READ2-(OVERLAP*2))-AMPLICONLENGTH")
print("\t\tExcess = ("+str(avgreadlen1)+"+"+str(avgreadlen2)+"-("+str(overlap)+"*2))-"+str(ampliconlength))

lengthstat = int(avgreads) - (int(overlap)*2) - int(ampliconlength)
print("\nASSESSMENT:")

#Data are correct
if (lengthstat > 10):
    print("\tOPTIMAL!")
    print("\tThe excess of nucleotides after considering amplicon, read, primer and overlap lengths is: " + str(lengthstat))
    print("\tTherefore, the parameters look correct to proceed with the analysis.\n")
#Data are in the edge
elif (lengthstat < 10) & (lengthstat >= 0):
    print("\tWARNING!")
    print("\tThe excess of nucleotides after considering amplicon, read, primer and overlap lengths is: " + str(lengthstat))
    print("\tThis means that the parameters are in the borderline for correct data processing. BAMSE will")
    print("\tcontinue with the pipeline, yet you should carefully analyse each step, and consider reducing")
    print("\toverlap and increasing MEE values if too many reads are lost in the data pre-processing.\n")
#Data are incorrect
else:
    print("\tERROR!")
    print("\tThere is no excess of nucleotides after considering amplicon, read, primer and overlap lengths,")
    print("\twhich means that the data cannot be processed with the current parameters, because there is no")
    print("\toverlap between forward and reverse reads. Doublecheck the introduced data (amplicon length, overlap)")
    print("\tare correct and run BAMSE again.\n")
    sys.exit(0)

#####################################################
################## Begin workflows ##################
#####################################################

##############################
# Primer clipping
##############################

if not all(list(map(os.path.isfile,primlist))):
       if (primF == 'noprimer') & (primR == 'noprimer'):
           print("\n{0} | SKIPPING PRIMER CLIPPING".format(current_time))
           print("     As no primer sequence has been declared, BAMSE considers primers have been")
           print("already clipped from the reads. If you want to clip primers from the read")
           print("declare primer sequences in flags -f and -r :")
           print("      e.g. -f CTANGGGNNGCANCAG -r GACTACNNGGGTATCTAAT.")
           # Output log to logfile
           logfile=open(log,"a+")
           current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
           logfile.write("\n{0} | Output files of the preprocessing workflow exist. \r\n".format(current_time))
           logfile.close()

       else:
           print("\n{0} | CLIPPING PRIMER SEQUENCES".format(current_time))
           print("     BAMSE will use Cutadapt to clip the forward and reverse primers from")
           print("the paired end reads. It will automatically detect if all sequences are")
           print("directional (output of PCR-based libraries) or not (output of ligation-based")
           print("libraries), and it will flip the reversed reads in the case of the latter.")
           print("Primer-trimmed files will be stored in the folder '1-Primertrimmed'.\n")
           # Output log to logfile
           logfile=open(log,"a+")
           current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
           logfile.write("\n{0} | Output files of the preprocessing workflow exist. \r\n".format(current_time))
           logfile.close()

           # Define snakefile
           path_snkf = os.path.join(bamsepath,'workflows/primertrimming/Snakefile')
           # Transform output file list into space-separated string (only for development)
           out_primertrim = " ".join(primlist)
           # Run snakemake
           prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_primertrim+' --configfile '+param+' --cores '+str(cores)+' -q'
           subprocess.Popen(prep_snk_Cmd, shell=True).wait()
else:
       # Output log to logfile
       logfile=open(log,"a+")
       current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
       logfile.write("\n{0} | Output files of the preprocessing workflow exist. \r\n".format(current_time))
       logfile.close()
       print("\n{0} | SKIPPING PRIMER CLIPPING".format(current_time))
       print("     The output files of the preprocessing workflow exist in the project")
       print("directory. If you want to re-run this step, remove the files in the directories")
       print("'1-Primertrimmed' and '2-Filtered'.\n")

##############################
# Calculate optimal trimming scores, filter and trim
##############################

if not all(list(map(os.path.isfile,filtlist))):
    prepropath=path+'/2-Filtered/'
    if not os.path.exists(prepropath):
        os.makedirs(prepropath)

    if (primF == 'noprimer') & (primR == 'noprimer'):
        if (trimfor == 'auto') & (trimrev == 'auto'):
            #Calculate optimal trimming scores from primer-clipped files
            print("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES".format(current_time))
            print("     As no primer sequence has been declared, BAMSE will use the raw")
            print("reads for calculating the optimal trimming scores with a minimum overlap")
            print("value of "+str(overlap)+" and maximum expected error value of MEE="+str(mee)+".")
            print("A random sample of "+str(maxreads)+" reads per sample will be used for the analysis.")
            print("This step can take a while if the size of the number and size of the files is large.\n")
            # Output log to logfile
            logfile=open(log,"a+")
            current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
            logfile.write("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES \r\n".format(current_time))
            logfile.close()

            # Define snakefile
            path_snkf = os.path.join(bamsepath,'workflows/findscores2/Snakefile')
            #Transform output file list into space-separated string (only for development)
            out_preprocessing = " ".join(csvlist)

            # Run find scores snakemake
            prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_preprocessing+' --configfile '+param+' --cores '+str(cores)+' -q'
            subprocess.Popen(prep_snk_Cmd, shell=True).wait()

            #### Find best score ####
            bastscore_Cmd = 'Rscript '+bamsepath+'/bin/bamse-optimaltrim.R -i '+path+'/2-Filtered/ -o '+path+'/2-Filtered/trim.txt -p '+param+' -l '+log+' 2>> /dev/null'
            subprocess.Popen(bastscore_Cmd, shell=True).wait()

            # Print best score
            trimparameters = []
            trimfile=path+'/2-Filtered/trim.txt'
            with open(trimfile, 'r') as arch:
                for line in arch:
                    trimparameters.append(eval(line.rstrip()))
            forwardtrim=trimparameters[0]
            reversetrim=trimparameters[1]

            print("\nThe analysis of optimal trimming parameters concluded that:")
            print("\tThe optimal position to trim Forward reads is at position "+str(forwardtrim)+".")
            print("\tThe optimal position to trim Reverse reads is at position "+str(reversetrim)+".")

            print("\n{0} | TRIMMING AND FILTERING READS".format(current_time))
            print("     BAMSE will trim the forward reads at position "+str(forwardtrim)+" and reverse reads")
            print("at position "+str(reversetrim)+", before calculating expected error values and filtering out")
            print("the reads with MEE values under "+str(mee)+".\n")
        else:
            # Filter and trim
            print("\n{0} | TRIMMING AND FILTERING READS".format(current_time))
            print("     BAMSE will trim the forward reads at position "+str(trimfor)+" and reverse")
            print("reads at position "+str(trimrev)+", before calculating expected error values")
            print("and filtering out the reads with MEE values under "+str(mee)+". Trimming parameters have")
            print("been specified by the user. If you want to find the optimal values automatically")
            print("do not specify the -s and -z flags.\n")
            trimfile=path+'/2-Filtered/trim.txt'
            trimlist=[trimfor,trimrev]
            f=open(trimfile,'w')
            for ele in trimlist:
                f.write(ele+'\n')
            f.close()

        # Filter and trim
        filterandtrim_Cmd = 'Rscript '+bamsepath+'/bin/bamse-filterandtrim.R -i '+path+'/0-Data/ -o '+path+'/2-Filtered/ -t '+path+'/2-Filtered/trim.txt -e '+mee+''
        subprocess.Popen(filterandtrim_Cmd, shell=True).wait()

        #Calculate number of reads after filtering and save to stats file
        for samplefile in filtlist:
            if samplefile.endswith("_1.fastq"):
                samplename=re.search('2-Filtered/(.*)/(.*)_1.fastq', samplefile).group(2)
                readnum=sum(1 for line in open(samplefile)) / 4
                statfile=path+'/0-Stats/'+samplename+'.txt'
                with open(statfile,'a') as f:
                    f.write('Filtered reads:\t'+str(int(readnum))+'\n')

    else:
        if (trimfor == 'auto') & (trimrev == 'auto'):
            #Calculate optimal trimming scores from original files
            print("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES".format(current_time))
            print("     BAMSE will use primer-clipped reads for calculating the optimal")
            print("trimming scores with a minimum overlap value of "+str(overlap)+" and a maximum")
            print("expected error of MEE="+str(mee)+". A random sample of "+str(maxreads)+" reads")
            print("per sample will be used for the analysis. This step can take a while if the size")
            print("of the number and size of the files is large.\n")
            # Output log to logfile
            logfile=open(log,"a+")
            current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
            logfile.write("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES \r\n".format(current_time))
            logfile.close()

            path_snkf = os.path.join(bamsepath,'workflows/findscores1/Snakefile')
            #Transform output file list into space-separated string (only for development)
            out_preprocessing = " ".join(csvlist)

            # Run snakemake
            prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_preprocessing+' --configfile '+param+' --cores '+str(cores)+' -q'
            subprocess.Popen(prep_snk_Cmd, shell=True).wait()

            #### Find best score ####
            bastscore_Cmd = 'Rscript '+bamsepath+'/bin/bamse-optimaltrim.R -i '+path+'/2-Filtered/ -o '+path+'/2-Filtered/trim.txt -p '+param+' -l '+log+' 2>> /dev/null'
            subprocess.Popen(bastscore_Cmd, shell=True).wait()

            # Print best score
            trimparameters = []
            trimfile=path+'/2-Filtered/trim.txt'
            with open(trimfile) as f:
                content = f.readlines()
            trimparameters = [x.strip() for x in content]
            forwardtrim=trimparameters[0]
            reversetrim=trimparameters[1]
            method=trimparameters[2]

            if (method == 'average'):
                print("\nWARNING! The analysis of optimal trimming parameters concluded that:")
                print("\tOptimal trimming lengths cannot be calculated because the read length is too short.")
                print("\tTry shortening the read overlap length or manual trimming position assessment.")
                print("\tBAMSE will continue  trimming the reads at their average read-length.")

                # Filter and trim
                print("\n{0} | FILTERING READS (WITHOUT TRIMMING)".format(current_time))
                print("     BAMSE will calculatEexpected error values and filter out ")
                print("the reads with MEE values under "+str(mee)+".\n")

            else:
                print("\nThe analysis of optimal trimming parameters concluded that:")
                print("\tThe optimal position to trim Forward reads is at position "+str(forwardtrim)+".")
                print("\tThe optimal position to trim Reverse reads is at position "+str(reversetrim)+".")

                # Filter and trim
                print("\n{0} | TRIMMING AND FILTERING READS".format(current_time))
                print("     BAMSE will trim the forward reads at position "+str(forwardtrim)+" and")
                print("reverse reads at position "+str(reversetrim)+", before calculating expected error values")
                print("and filtering out the reads with MEE values under "+str(mee)+".\n")
        else:
            # Filter and trim
            print("\n{0} | TRIMMING AND FILTERING READS".format(current_time))
            print("     BAMSE will trim the forward reads at position "+str(trimfor)+" and reverse")
            print("reads at position "+str(trimrev)+", before calculating expected error values")
            print("and filtering out the reads with MEE values under "+str(mee)+". Trimming parameters have")
            print("been specified by the user. If you want to find the optimal values automatically")
            print("do not specify the -s and -z flags.\n")
            trimfile=path+'/2-Filtered/trim.txt'
            trimlist=[trimfor,trimrev]
            f=open(trimfile,'w')
            for ele in trimlist:
                f.write(ele+'\n')
            f.close()

        filterandtrim_Cmd = 'Rscript '+bamsepath+'/bin/bamse-filterandtrim.R -i '+path+'/1-Primertrimmed/ -o '+path+'/2-Filtered/ -t '+path+'/2-Filtered/trim.txt -e '+str(mee)+''
        subprocess.Popen(filterandtrim_Cmd, shell=True).wait()

        #Calculate number of reads after filtering and save to stats file
        for samplefile in filtlist:
            if samplefile.endswith("_1.fastq"):
                print(samplefile)
                samplename=re.search('2-Filtered/(.*)/(.*)_1.fastq', samplefile).group(2)
                readnum=sum(1 for line in open(samplefile)) / 4
                statfile=path+'/0-Stats/'+samplename+'.txt'
                with open(statfile,'a') as f:
                    f.write('Filtered reads:\t'+str(int(readnum))+'\n')

else:
       # Output log to logfile
       print("\n{0} | SKIPPING READ FILTERING AND TRIMMING".format(current_time))
       print("     The output files of the read trimming and filtering step exist in the project")
       print("directory. If you want to re-run this step, remove the files in the directory")
       print("'2-Filtered'.\n")
       logfile=open(log,"a+")
       current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
       logfile.write("\n{0} | Output files of the trimming and filtering stepexist. \r\n".format(current_time))
       logfile.close()


########################
# Run DADA2 A workflow #     Error learning and dada2 of each run individually
########################

# Define output names
dadalist2 = list(dict.fromkeys(dadalist))
out_dada2a = " ".join(dadalist2)
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2a/Snakefile')

if all(list(map(os.path.isfile,dadalist2))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the DADA2 A workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING DADA2 ERROR LEARNING AND ASV GENERATION WORKFLOW.".format(current_time))
    print("     The output files of the DADA2 error learning and ASV generation workflow")
    print("exist in the project directoy. If you want to re-run this step, remove the files ")
    print("in the directory '3-DADA2'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running DADA2 error learning \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING DADA2 ERROR LEARNING AND ASV GENERATION WORKFLOW.".format(current_time))
    print("     BAMSE will run DADA2 for learning errors from a subset of")
    print("samples and correct sequencing errors from all files before generating ASVs.")
    print("In large datasets this step might take a while.\n")


# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2a+' --configfile '+param+' --cores '+str(cores)+' -q'
subprocess.Popen(prep_snk_Cmd, shell=True).wait()

########################
# Run DADA2 B workflow #      Merge the different runs, and output results
########################

# Define output names
out_dada2b = path+'/ASV_counts.csv '+path+'/ASVs.fasta '+path+'/ASV_taxa.txt'
out_dada2b_list = out_dada2b.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2b/Snakefile')

if all(list(map(os.path.isfile,out_dada2b_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the DADA2 B workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING DADA2 CHIMERA FILTERING AND TAXONOMY ANNOTATION WORKFLOW.".format(current_time))
    print("     The output files of the DADA2 chimera filtering and taxonomy annotation workflow")
    print("exist in the project directoy. If you want to re-run this step, remove the files ")
    print("in the directory '4-Taxonomyfilter', as well as the files 'ASVs.fasta', 'ASV_counts.csv'")
    print("and 'ASV_taxa.txt'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running DADA2 chimera filtering and taxonomy annotation \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING DADA2 CHIMERA FILTERING AND TAXONOMY ANNOTATION WORKFLOW.".format(current_time))
    print("     BAMSE will run DADA2 for filtering chimeric sequences and assigning")
    print("taxonomy to the ASVs. BAMSE will only retain ASVs assigned at least to a")
    print("Bacteria/Archaea Phylum level. In large datasets this step might take a while.\n")

    taxonfilterpath=path+'/4-Taxonomyfilter/'
    if not os.path.exists(taxonfilterpath):
        os.makedirs(taxonfilterpath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2b+' --configfile '+param+' --cores '+str(cores)+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()

#####################
# Run LULU workflow #      Curate ASV list and resulting ASV table
#####################

if luluarg == True:

    lulupath=path+'/5-LULU/'
    if not os.path.exists(taxonfilterpath):
        os.makedirs(taxonfilterpath)

    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running the LULU curation \r\n".format(current_time))
    logfile.close()
    print("\n{0} | RUNNING LULU CURATION.".format(current_time))
    print("     BAMSE will run LULU, because you have specified the flag -u.")
    print("Filtered ASV sequence document and tables will be named as")
    print("ASVs.lulu.fasta and ASV_counts.lulu.csv, respectively.\n")

    # Define output names
    out_lulu = path+'/ASV_counts.lulu.csv '+path+'/ASVs.lulu.fasta'
    curr_dir = os.path.dirname(sys.argv[0])
    bamsepath = os.path.abspath(curr_dir)
    path_snkf = os.path.join(bamsepath,'workflows/lulu/Snakefile')

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_lulu+' --configfile '+param+' --cores '+str(cores)+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()


#########################################
# Common contaminant detection workflow #
#########################################



###############################
# Run ASV clustering workflow #
###############################




#####################
# Run ASV phylogeny #
#####################

# Define output names
out_phylogeny = path+'/6-Phylogeny/ASVs.align.fasta '+path+'/ASVs.tre'
out_phylogeny_list = out_phylogeny.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/phylogeny/Snakefile')

if all(list(map(os.path.isfile,out_phylogeny_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the ASV phylogeny generation workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING ASV PHYLOGENY GENERATION WORKFLOW.".format(current_time))
    print("     The output files of the ASV phylogeny generation workflow exist. If you want to")
    print("re-run this step, remove the files in the directory '6-Phylogeny', as well as the")
    print("file 'ASVs.tre'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running ASV phylogeny \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING ASV PHYLOGENY GENERATION WORKFLOW.".format(current_time))
    print("     BAMSE will first align the ASV sequences to then generate a Maximum")
    print("Likelihood phylogenetic tree. This step might take several hours when the")
    print("number of ASVs is high.\n")

    phylopath=path+'/6-Phylogeny/'
    if not os.path.exists(phylopath):
        os.makedirs(phylopath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_phylogeny+' --configfile '+param+' --cores '+str(cores)+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()

############################
# Run OTU binning workflow #
############################

# Define output names
out_binning = path+'/7-Binning/binmap.txt '+path+'/7-Binning/ASVs.counts.fasta '+path+'/7-Binning/ASVs.sorted.fasta '+path+'/7-Binning/bintable.txt '+path+'/ASVs.binned.fasta '+path+'/ASV_counts.binned.csv '+path+'/ASVs.binned.tre'
out_binning_list = out_binning.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/binning/Snakefile')

if all(list(map(os.path.isfile,out_binning_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the ASV binning workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING ASV BINNING WORKFLOW.".format(current_time))
    print("     The output files of the ASV binning workflow exist. If you want to re-run")
    print("this step, remove the files in the directory '7-Binning', as well as the")
    print("files 'ASVs.binned.fasta', 'ASV_counts.binned.csv' and 'ASVs.binned.tre'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running ASV binning \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING ASV BINNING WORKFLOW.".format(current_time))
    print("     BAMSE will bin ASVs with sequence identity of >97%. The binned ASV ")
    print("sequence file, ASV count table and ASV phylogenetic tree will be named")
    print("ASVs.binned.fasta, ASV_counts.binned.csv and ASV.binned.tre, respectively.\n")

    binpath=path+'/7-Binning/'
    if not os.path.exists(binpath):
        os.makedirs(binpath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_binning+' --configfile '+param+' --cores '+str(cores)+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()


###################################
# Run diversity analysis workflow #
###################################

###################################
# Merge stats files into one file #
###################################

if all(list(map(os.path.isfile,out_binning_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("{0} | BAMSE has completed succesfully \r\n".format(current_time))
    logfile.close()
    print("{0} | BAMSE HAS COMPLETED SUCCESFULLY! \r\n".format(current_time))
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("{0} | Not all expected files have been created. Check logfile. \r\n".format(current_time))
    logfile.close()
    print("{0} | SOME EXPECTED FILES HAVE NOT BEEN CREATED!".format(current_time))
    print("Check the logfile for more information.")
