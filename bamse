#!/usr/bin/env python

import os
import sys

####################
# Check if conda environment bamse-env is active #
####################

syspath = sys.prefix

if not "bamse-env" in syspath:
    print("#####################")
    print("#### BAMSE v1.0 ##### ERROR!")
    print("#####################")
    print("\nThe conda environment bamse-env is not activated\nType the following code before running bamse:\n\tconda activate bamse-env\n")
    sys.exit(0)

import argparse
import subprocess
import ruamel.yaml
import pathlib
import re
import time
from shutil import which

####################
# Argument parsing #
####################
# add overwrite option -w

parser = argparse.ArgumentParser(description='Runs BAMSE pipeline.')
parser.add_argument('-i', help="Data information file", dest="input", required=True)
parser.add_argument('-d', help="Working directory of the project", dest="workdir", required=True)
parser.add_argument('-f', help="Forward primer sequence", dest="primF", required=True)
parser.add_argument('-r', help="Reverse primer sequence", dest="primR", required=True)
parser.add_argument('-a', help="Expected average amplicon length", dest="ampliconlength", required=True)
parser.add_argument('-x', help="Absolute path to the taxonomy database", dest="tax", required=True)
parser.add_argument('-t', help="Number of threads", dest="threads", required=True)
parser.add_argument('-e', help="Maximum number of expected errors per read", dest="mee", required=False)
parser.add_argument('-o', help="Minimum overlap between reads for merging", dest="overlap", required=False)
parser.add_argument('-y', help="Taxonomy filtering threshold", dest="taxfilter", required=False)
parser.add_argument('-m', help="Minimum fold to consider parent ASVs for chimera detection", dest="fold", required=False)
parser.add_argument('-u', help="Whether to run LULU clustering", dest="lulu", required=False ,action='store_true')
parser.add_argument('-p', help="Absolute path to the parameters file that BAMSE will create", dest="param", required=False)
parser.add_argument('-l', help="Absolute path to the log file that BAMSE will create", dest="log", required=False)
parser.add_argument('-w', help="Overwrite contents in the working directory of the project", action='store_true', dest="overwrite", required=False)
args = parser.parse_args()

# Translate arguments
in_f=args.input
path=args.workdir
primF=args.primF
primR=args.primR
ampliconlength=args.ampliconlength
tax=args.tax
fold=args.fold
cores=args.threads
overwrite=args.overwrite
luluarg=args.lulu

# Retrieve current directory
file = os.path.dirname(sys.argv[0])
curr_dir = os.path.abspath(file)

# Remove contents in the directory
#if overwrite == True:
    #Add removing commands

# Create workign directory if does not exist
if not os.path.exists(path):
    os.makedirs(path)

#Remove last / to the working directory (if necessary)
path = re.sub('/$','',path)

# Define param file
if not (args.param):
    param=os.path.join(os.path.abspath(path),"bamse.yaml")
else:
    param=args.param

# Define log file
if not (args.log):
    log=os.path.join(path,"bamse.log")
else:
    log=args.log

# Define quality filtering value
if not (args.taxfilter):
    taxfilter='kingdom'
else:
    taxfilter=args.taxfilter

# Define quality filtering value
if not (args.mee):
    mee=2
else:
    mee=args.mee

# Define overlap  value
if not (args.overlap):
    overlap=5
else:
    overlap=args.overlap

# Define quality filtering value
if not (args.fold):
    fold=1
else:
    fold=args.fold

#Remove param file if exists
if os.path.exists(param):
    os.remove(param)

curr_dir=os.path.dirname(sys.argv[0])
bamsepath=os.path.abspath(curr_dir)

###################
# Create log file #
###################

logfile=open(log,"w+")
logfile.write("#####################\n")
logfile.write("#### BAMSE v1.0 #####\n")
logfile.write("#####################\n")
logfile.write("BAMSE is starting at:\n")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\t{0}\n".format(current_time))
logfile.close()

#Print on screen
print("#####################")
print("#### BAMSE v1.0 #####")
print("#####################\n")
print("BAMSE is starting at:")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
print("\t{0}".format(current_time))

#############################################
# Append information to the parameters file #
#############################################

#Append information to the parameters file
f = open(str(param), "a")
f.write("#BAMSE core paths\n")
f.write("bamsepath:\n "+str(curr_dir)+"\n")
f.write("projectpath:\n "+str(path)+"\n")
f.write("parampath:\n "+str(param)+"\n")
f.write("logpath:\n "+str(log)+"\n")
f.write("taxonomy:\n "+str(tax)+"\n")
f.write("\n#Primers\n")
f.write("primer1:\n "+str(primF)+"\n")
f.write("primer2:\n "+str(primR)+"\n")
f.write("\n#Trimming and filtering\n")
f.write("ampliconlength:\n "+str(ampliconlength)+"\n")
f.write("mee:\n "+str(mee)+"\n")
f.write("overlap:\n "+str(overlap)+"\n")
f.write("\n#Chimera filtering\n")
f.write("fold:\n "+str(fold)+"\n")
f.write("\n#Taxonomy filtering\n")
f.write("taxfilter:\n "+str(taxfilter)+"\n")
f.close()

#Append information to the log file
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\nBAMSE is running with the following parameters:\n")
logfile.write("#Paths\n")
logfile.write("\tbamsepath: "+str(curr_dir)+"\n")
logfile.write("\tprojectpath: "+str(path)+"\n")
logfile.write("\tparampath: "+str(param)+"\n")
logfile.write("\tlogpath: "+str(log)+"\n")
logfile.write("\ttaxonomy: "+str(tax)+"\n")
logfile.write("#Primers\n")
logfile.write("\tprimer1: "+str(primF)+"\n")
logfile.write("\tprimer2: "+str(primR)+"\n")
logfile.write("#Parameters\n")
logfile.write("\tampliconlength: "+str(ampliconlength)+"\n")
logfile.write("\tmee: "+str(mee)+"\n")
logfile.write("\toverlap: "+str(overlap)+"\n")
logfile.write("\tfold: "+str(fold)+"\n")
logfile.write("taxfilter:\n "+str(taxfilter)+"\n")
logfile.close()

#Print information on screen
print("\nBAMSE is running with the following parameters:\n")
print("PATHS")
print("\tBamse path: "+str(curr_dir))
print("\tProject path: "+str(path))
print("\tParameters path: "+str(param))
print("\tLog path: "+str(log))
print("\tTaxonomy database path: "+str(tax)+"\n")
print("PRIMERS")
print("\tForward primer: "+str(primF))
print("\tReverse primer: "+str(primR)+"\n")
print("PARAMETERS")
print("\tAmplicon length: "+str(ampliconlength))
print("\tMaximum Expected Error: "+str(mee))
print("\tOverlap: "+str(overlap))
print("\tfold: "+str(fold))
print("\ttaxfilter: "+str(taxfilter)+"\n")
print("##########################################")

######################
# Check dependencies #
######################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Checking dependencies \r\n".format(current_time))
print("\n{0} | Checking dependencies:".format(current_time))

def is_tool(name):
    return which(name) is not None

if not is_tool('perl'):
    logfile.write("\tERROR! Perl is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('snakemake'):
    logfile.write("\tERROR! Snakemake is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('R'):
    logfile.write("\tERROR! R is not installed or loaded. \n")
    sys.exit(0)

else:
    logfile.write("\tAll dependencies are properly installed. \n")
    print("\tAll dependencies are properly installed.")
    logfile.close()


#########################################
# Check if input information is correct #
#########################################

if not os.path.isfile(in_f):
    logfile.write("\tERROR! The input data file does not exist. \n")
    print("ERROR! The input data file does not exist.")
    sys.exit(0)

if not os.path.isfile(tax):
    logfile.write("\tERROR! The taxonomy database path is incorrect. \n")
    print("ERROR! The taxonomy database path is incorrect.")
    sys.exit(0)

if not (taxfilter == 'kingdom') and (taxfilter == 'phylum') and (taxfilter == 'class') and (taxfilter == 'order') and (taxfilter == 'family') and (taxfilter == 'genus'):
    logfile=open(log,"a+")
    logfile.write("\tERROR! The assigned taxonomy filter " + taxfilter + " is incorrect.\n")
    logfile.write("\tThe allowed values include: 'division', 'phylum', 'class', 'order', 'family' and 'genus'.\n")
    logfile.close()
    print("\tERROR! The assigned taxonomy parameter " + taxfilter + " is incorrect")
    print("\tThe allowed values include: 'division', 'phylum', 'class', 'order', 'family' and 'genus.")
    sys.exit(0)

###############################
# Prepare working directories #
###############################

# Set input directory
dir0 = os.path.join(path,"0-Data")
#dir1 = os.path.join(path,"1-Trimmed")
#dir2 = os.path.join(path,"2-Filtered")

## If input directory does not exist, make it
if not os.path.exists(dir0):
    os.makedirs(dir0)

## If stats directory does not exist, make it
statsdir = os.path.join(path,"0-Stats")
if not os.path.exists(statsdir):
    os.makedirs(statsdir)

#################
# Transfer data #
#################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Transferring data to the working directory \r\n".format(current_time))
logfile.close()
print("\n{0} | Transferring data to the working directory".format(current_time))


####
# Add error if input data contains duplicated sequences
####

# Read input data file
inputfile = open(in_f, "r")

#Declare empty output file list
outlist = []
runlist = []

## Read input data row by row
for line in inputfile:
    ### Skip line if starts with # (comment line)
    if not (line.startswith('#')):
        ###Skip line if it's empty
        if not len(line.strip()) == 0 :
            #Define variables
            line=line.replace('\n', '')
            linelist = line.split(',') # Create a list of each line

            #Test whether the list contains 4 elements
            if len(linelist) != 4:
                logfile=open(log,"a+")
                logfile.write("\tThe format of the data input file is incorrect.\n")
                print("\tERROR! The format of the data input file is incorrect.")
                print("\tEnsure the file is comma-separated and contains")
                print("\tfour columns: SAMPLE, POOL, FORWARD, REVERSE. e.g.:")
                print("\tSAMPLE1,POOL1,Sample1_1.fq.gz,Sample1_2.fq.gz")
                print("\tSAMPLE2,POOL1,Sample2_1.fq.gz,Sample2_2.fq.gz\n")
                logfile.close()
                sys.exit(0)

            #Assign variables
            name=linelist[0]
            run=linelist[1]
            in_for=linelist[2]
            in_rev=linelist[3]

            #Create run folder if it does not exist
            runpath=path+'/0-Data/'+run
            if not os.path.exists(runpath):
                os.makedirs(runpath)

            #Create list of final files for workflow DADA2 A
            out_run = path+'/3-DADA2/'+run+'.rds'
            runlist.append(out_run)

            # Transfer, rename and decompress data

            #Check if the file is already in the working directory
            out1=runpath+'/'+name+'_1.fastq'
            if os.path.isfile(out1):
                logfile=open(log,"a+")
                logfile.write("\tThe file " + out1 + " is already in the working directory.\n")
                logfile.close()
            else:
                #If the file is not in the working directory, transfer it
                if os.path.isfile(in_for):
                    if in_for.endswith('.gz'):
                        read1Cmd = 'gunzip -c '+in_for+' > '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                    else:
                        read1Cmd = 'cp '+in_for+' '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_for + " does not exist.\n")
                    logfile.close()

            #Check if the file is already in the working directory
            out2=runpath+'/'+name+'_2.fastq'
            if os.path.isfile(out2):
                logfile=open(log,"a+")
                logfile.write("\tThe file " + out2 + " is already in the working directory.\n")
                logfile.close()
            else:
                #If the file is not in the working directory, transfer it
                if os.path.isfile(in_rev):
                    if in_for.endswith('.gz'):
                        read2Cmd = 'gunzip -c '+in_rev+' > '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                    else:
                        read2Cmd = 'cp '+in_rev+' '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_rev + " does not exist.\n")
                    logfile.close()

            #Create list of output files for preprocessing workflow
            out_for = path+'/2-Filtered/'+run+'/'+name+'_1.fastq'
            out_rev = path+'/2-Filtered/'+run+'/'+name+'_2.fastq'
            outlist.append(out_for)
            outlist.append(out_rev)

#####################################################
################## Begin workflows ##################
#####################################################

##############################
# Run preprocessing workflow #     Prepare data for DADA2 pipeline (trimming, quality filtering, etc.)
##############################

if all(list(map(os.path.isfile,outlist))):
       # Output log to logfile
       logfile=open(log,"a+")
       current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
       logfile.write("\n{0} | Output files of the preprocessing workflow exist. \r\n".format(current_time))
       logfile.close()
       print("\n{0} | SKIPPING PREPROCESSING WORKFLOW.".format(current_time))
       print("     The output files of the preprocessing workflow exist in the project")
       print("directoy. If you want to re-run this step, remove the files in the directories")
       print("'1-Primertrimmed' and '2-Filtered'.\n")
else:
       # Output log to logfile
       logfile=open(log,"a+")
       current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
       logfile.write("\n{0} | Starting preprocessing workflow \r\n".format(current_time))
       logfile.close()
       print("\n{0} | STARTING PREPROCESSING WORKFLOW.".format(current_time))
       print("     BAMSE will remove the primer sequences from both 5' ends of the forward")
       print("and reverse read. It will automatically detect if all sequences are")
       print("directional (output of PCR-based libraries) or not (output of ligation-based")
       print("libraries), and it will flip the reversed reads in the case of the latter.")
       print("Primer-trimmed files will be stored in the folder '1-Primertrimmed'.\n")
       print("     BAMSE will then trim low-quality 3' ends of the reads and will filter out")
       print("reads under the quality threshold specified. These preprocessed files will")
       print("be stored in the folder '2-Filtered'.\n")

       path_snkf = os.path.join(bamsepath,'workflows/preprocessing/Snakefile')
       #Transform output file list into space-separated string (only for development)
       out_preprocessing = " ".join(outlist)

       # Run snakemake
       prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_preprocessing+' --configfile '+param+' --cores '+cores+' -q'
       subprocess.Popen(prep_snk_Cmd, shell=True).wait()

########################
# Run DADA2 A workflow #     Error learning and dada2 of each run individually
########################

# Define output names
runlist2 = list(dict.fromkeys(runlist))
out_dada2a = " ".join(runlist2)
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2a/Snakefile')

if all(list(map(os.path.isfile,runlist2))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the DADA2 A workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING DADA2 ERROR LEARNING AND ASV GENERATION WORKFLOW.".format(current_time))
    print("     The output files of the DADA2 error learning and ASV generation workflow")
    print("exist in the project directoy. If you want to re-run this step, remove the files ")
    print("in the directory '3-DADA2'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running DADA2 error learning \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING DADA2 ERROR LEARNING AND ASV GENERATION WORKFLOW.".format(current_time))
    print("     BAMSE will run DADA2 for learning errors from a subset of")
    print("samples and correct sequencing errors from all files before generating ASVs.")
    print("In large datasets this step might take a while.\n")


# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2a+' --configfile '+param+' --cores '+cores+' -q'
subprocess.Popen(prep_snk_Cmd, shell=True).wait()

########################
# Run DADA2 B workflow #      Merge the different runs, and output results
########################

# Define output names
out_dada2b = path+'/ASV_counts.csv '+path+'/ASVs.fasta '+path+'/ASV_taxa.txt'
out_dada2b_list = out_dada2b.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2b/Snakefile')

if all(list(map(os.path.isfile,out_dada2b_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the DADA2 B workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING DADA2 CHIMERA FILTERING AND TAXONOMY ANNOTATION WORKFLOW.".format(current_time))
    print("     The output files of the DADA2 chimera filtering and taxonomy annotation workflow")
    print("exist in the project directoy. If you want to re-run this step, remove the files ")
    print("in the directory '4-Taxonomyfilter', as well as the files 'ASVs.fasta', 'ASV_counts.csv'")
    print("and 'ASV_taxa.txt'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running DADA2 chimera filtering and taxonomy annotation \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING DADA2 CHIMERA FILTERING AND TAXONOMY ANNOTATION WORKFLOW.".format(current_time))
    print("     BAMSE will run DADA2 for filtering chimeric sequences and assigning")
    print("taxonomy to the ASVs. BAMSE will only retain ASVs assigned at least to a")
    print("Bacteria/Archaea Phylum level. In large datasets this step might take a while.\n")

    taxonfilterpath=path+'/4-Taxonomyfilter/'
    if not os.path.exists(taxonfilterpath):
        os.makedirs(taxonfilterpath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2b+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()

#####################
# Run LULU workflow #      Curate ASV list and resulting ASV table
#####################

if luluarg == True:

    lulupath=path+'/5-LULU/'
    if not os.path.exists(taxonfilterpath):
        os.makedirs(taxonfilterpath)

    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running the LULU curation \r\n".format(current_time))
    logfile.close()
    print("\n{0} | RUNNING LULU CURATION.".format(current_time))
    print("     BAMSE will run LULU, because you have specified the flag -u.")
    print("Filtered ASV sequence document and tables will be named as")
    print("ASVs.lulu.fasta and ASV_counts.lulu.csv, respectively.\n")

    # Define output names
    out_lulu = path+'/ASV_counts.lulu.csv '+path+'/ASVs.lulu.fasta'
    curr_dir = os.path.dirname(sys.argv[0])
    bamsepath = os.path.abspath(curr_dir)
    path_snkf = os.path.join(bamsepath,'workflows/lulu/Snakefile')

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_lulu+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()


#########################################
# Common contaminant detection workflow #
#########################################



###############################
# Run ASV clustering workflow #
###############################




#####################
# Run ASV phylogeny #
#####################

# Define output names
out_phylogeny = path+'/6-Phylogeny/ASVs.align.fasta '+path+'/ASVs.tre'
out_phylogeny_list = out_phylogeny.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/phylogeny/Snakefile')

if all(list(map(os.path.isfile,out_phylogeny_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the ASV phylogeny generation workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING ASV PHYLOGENY GENERATION WORKFLOW.".format(current_time))
    print("     The output files of the ASV phylogeny generation workflow exist. If you want to")
    print("re-run this step, remove the files in the directory '6-Phylogeny', as well as the")
    print("file 'ASVs.tre'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running ASV phylogeny \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING ASV PHYLOGENY GENERATION WORKFLOW.".format(current_time))
    print("     BAMSE will first align the ASV sequences to then generate a Maximum")
    print("Likelihood phylogenetic tree. This step might take several hours when the")
    print("number of ASVs is high.\n")

    phylopath=path+'/6-Phylogeny/'
    if not os.path.exists(phylopath):
        os.makedirs(phylopath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_phylogeny+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()

############################
# Run OTU binning workflow #
############################

# Define output names
out_binning = path+'/7-Binning/binmap.txt '+path+'/7-Binning/ASVs.counts.fasta '+path+'/7-Binning/ASVs.sorted.fasta '+path+'/7-Binning/bintable.txt '+path+'/ASVs.binned.fasta '+path+'/ASV_counts.binned.csv '+path+'/ASVs.binned.tre'
out_binning_list = out_binning.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/binning/Snakefile')

if all(list(map(os.path.isfile,out_binning_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the ASV binning workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING ASV BINNING WORKFLOW.".format(current_time))
    print("     The output files of the ASV binning workflow exist. If you want to re-run")
    print("this step, remove the files in the directory '7-Binning', as well as the")
    print("files 'ASVs.binned.fasta', 'ASV_counts.binned.csv' and 'ASVs.binned.tre'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running ASV binning \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING ASV BINNING WORKFLOW.".format(current_time))
    print("     BAMSE will bin ASVs with sequence identity of >97%. The binned ASV ")
    print("sequence file, ASV count table and ASV phylogenetic tree will be named")
    print("ASVs.binned.fasta, ASV_counts.binned.csv and ASV.binned.tre, respectively.\n")

    binpath=path+'/7-Binning/'
    if not os.path.exists(binpath):
        os.makedirs(binpath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_binning+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()


###################################
# Run diversity analysis workflow #
###################################

###################################
# Merge stats files into one file #
###################################

finalist=outlist+runlist2+out_dada2b_list+out_phylogeny_list+out_binning_list

if all(list(map(os.path.isfile,out_binning_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("{0} | BAMSE has completed succesfully \r\n".format(current_time))
    logfile.close()
    print("{0} | BAMSE HAS COMPLETED SUCCESFULLY! \r\n".format(current_time))
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("{0} | Not all expected files have been created. Check logfile. \r\n".format(current_time))
    logfile.close()
    print("{0} | SOME EXPECTED FILES HAVE NOT BEEN CREATED!".format(current_time))
    print("Check the logfile for more information.")
