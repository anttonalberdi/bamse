#!/usr/bin/env python

import os
import sys

####################
# Check if conda environment bamse-env is active #
####################

syspath = sys.prefix

if not "bamse-env" in syspath:
    print("#####################")
    print("#### BAMSE v1.0 ##### ERROR!")
    print("#####################")
    print("\nThe conda environment bamse-env is not activated\nType the following code before running bamse:\n\tconda activate bamse-env\n")
    sys.exit(0)

import argparse
import subprocess
import ruamel.yaml
import pathlib
import re
import time
from shutil import which

####################
# Argument parsing #
####################
# add overwrite option -w

parser = argparse.ArgumentParser(description='Runs BAMSE pipeline.')
parser.add_argument('-i', help="Data information file", dest="input", required=True)
parser.add_argument('-d', help="Working directory of the project", dest="workdir", required=True)
parser.add_argument('-f', help="Forward primer sequence", dest="primF", required=False)
parser.add_argument('-r', help="Reverse primer sequence", dest="primR", required=False)
parser.add_argument('-a', help="Expected average amplicon length", dest="ampliconlength", required=True)
parser.add_argument('-x', help="Absolute path to the taxonomy database", dest="tax", required=True)
parser.add_argument('-t', help="Number of threads", dest="threads", required=True)
parser.add_argument('-e', help="Maximum number of expected errors per read", dest="mee", required=False)
parser.add_argument('-o', help="Minimum overlap between reads for merging", dest="overlap", required=False)
parser.add_argument('-y', help="Taxonomy filtering threshold", dest="taxfilter", required=False)
parser.add_argument('-m', help="Minimum fold to consider parent ASVs for chimera detection", dest="fold", required=False)
parser.add_argument('-u', help="Whether to run LULU clustering", dest="lulu", required=False ,action='store_true')
parser.add_argument('-p', help="Absolute path to the parameters file that BAMSE will create", dest="param", required=False)
parser.add_argument('-l', help="Absolute path to the log file that BAMSE will create", dest="log", required=False)
parser.add_argument('-w', help="Overwrite contents in the working directory of the project", action='store_true', dest="overwrite", required=False)
args = parser.parse_args()

# Translate arguments
in_f=args.input
path=args.workdir
ampliconlength=args.ampliconlength
tax=args.tax
fold=args.fold
cores=args.threads
overwrite=args.overwrite
luluarg=args.lulu

# Retrieve current directory
file = os.path.dirname(sys.argv[0])
curr_dir = os.path.abspath(file)

# Remove contents in the directory
#if overwrite == True:
    #Add removing commands

# Create workign directory if does not exist
if not os.path.exists(path):
    os.makedirs(path)

#Remove last / to the working directory (if necessary)
path = re.sub('/$','',path)

#Define primers
if not (args.primF):
    primF='noprimer'
else:
    primF=args.primF

if not (args.primR):
    primR='noprimer'
else:
    primR=args.primR

# Define param file
if not (args.param):
    param=os.path.join(os.path.abspath(path),"bamse.yaml")
else:
    param=args.param

# Define log file
if not (args.log):
    log=os.path.join(path,"bamse.log")
else:
    log=args.log

# Define quality filtering value
if not (args.taxfilter):
    taxfilter='kingdom'
else:
    taxfilter=args.taxfilter

# Define quality filtering value
if not (args.mee):
    mee=2
else:
    mee=args.mee

# Define overlap value
if not (args.overlap):
    overlap=20
else:
    overlap=args.overlap

# Define quality filtering value
if not (args.fold):
    fold=1
else:
    fold=args.fold

#Remove param file if exists
if os.path.exists(param):
    os.remove(param)

curr_dir=os.path.dirname(sys.argv[0])
bamsepath=os.path.abspath(curr_dir)

###################
# Create log file #
###################

logfile=open(log,"w+")
logfile.write("#####################\n")
logfile.write("#### BAMSE v1.0 #####\n")
logfile.write("#####################\n")
logfile.write("BAMSE is starting at:\n")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\t{0}\n".format(current_time))
logfile.close()

#Print on screen
print("\n#####################")
print("#### BAMSE v1.0 #####")
print("#####################\n")
print("BAMSE is starting at:")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
print("\t{0}".format(current_time))

#############################################
# Append information to the parameters file #
#############################################

#Append information to the parameters file
f = open(str(param), "a")
f.write("#BAMSE core paths\n")
f.write("bamsepath:\n "+str(curr_dir)+"\n")
f.write("projectpath:\n "+str(path)+"\n")
f.write("parampath:\n "+str(param)+"\n")
f.write("logpath:\n "+str(log)+"\n")
f.write("taxonomy:\n "+str(tax)+"\n")
f.write("\n#Primers\n")
f.write("primer1:\n "+str(primF)+"\n")
f.write("primer2:\n "+str(primR)+"\n")
f.write("\n#Trimming and filtering\n")
f.write("ampliconlength:\n "+str(ampliconlength)+"\n")
f.write("mee:\n "+str(mee)+"\n")
f.write("overlap:\n "+str(overlap)+"\n")
f.write("\n#Chimera filtering\n")
f.write("fold:\n "+str(fold)+"\n")
f.write("\n#Taxonomy filtering\n")
f.write("taxfilter:\n "+str(taxfilter)+"\n")
f.close()

#Append information to the log file
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\nBAMSE is running with the following parameters:\n")
logfile.write("#Paths\n")
logfile.write("\tbamsepath: "+str(curr_dir)+"\n")
logfile.write("\tprojectpath: "+str(path)+"\n")
logfile.write("\tparampath: "+str(param)+"\n")
logfile.write("\tlogpath: "+str(log)+"\n")
logfile.write("\ttaxonomy: "+str(tax)+"\n")
logfile.write("#Primers\n")
logfile.write("\tprimer1: "+str(primF)+"\n")
logfile.write("\tprimer2: "+str(primR)+"\n")
logfile.write("#Parameters\n")
logfile.write("\tampliconlength: "+str(ampliconlength)+"\n")
logfile.write("\tmee: "+str(mee)+"\n")
logfile.write("\toverlap: "+str(overlap)+"\n")
logfile.write("\tfold: "+str(fold)+"\n")
logfile.write("taxfilter:\n "+str(taxfilter)+"\n")
logfile.close()

#Print information on screen
print("\nBAMSE is running with the following parameters:\n")
print("PATHS")
print("\tBamse path: "+str(curr_dir))
print("\tProject path: "+str(path))
print("\tParameters path: "+str(param))
print("\tLog path: "+str(log))
print("\tTaxonomy database path: "+str(tax)+"\n")
print("PRIMERS")
print("\tForward primer: "+str(primF))
print("\tReverse primer: "+str(primR)+"\n")
print("PARAMETERS")
print("\tAmplicon length: "+str(ampliconlength))
print("\tMaximum Expected Error: "+str(mee))
print("\tOverlap: "+str(overlap))
print("\tfold: "+str(fold))
print("\ttaxfilter: "+str(taxfilter)+"\n")
print("If you don't know what each of these parameters mean visit:")
print("\thttps://github.com/anttonalberdi/bamse\n")
print("##########################################")

######################
# Check dependencies #
######################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Checking dependencies \r\n".format(current_time))
print("\n{0} | CHECKING DEPENDENCIES".format(current_time))

def is_tool(name):
    return which(name) is not None

if not is_tool('perl'):
    logfile.write("\tERROR! Perl is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('snakemake'):
    logfile.write("\tERROR! Snakemake is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('R'):
    logfile.write("\tERROR! R is not installed or loaded. \n")
    sys.exit(0)

else:
    logfile.write("\tAll dependencies are properly installed. \n")
    print("\tAll dependencies are properly installed.")
    logfile.close()


#########################################
# Check if input information is correct #
#########################################

if not os.path.isfile(in_f):
    print("ERROR! The input data file does not exist.")
    logfile.write("\tERROR! The input data file does not exist. \n")
    sys.exit(0)

if not os.path.isfile(tax):
    print("ERROR! The taxonomy database path is incorrect.")
    logfile.write("\tERROR! The taxonomy database path is incorrect. \n")
    sys.exit(0)

if not (taxfilter == 'kingdom') and (taxfilter == 'phylum') and (taxfilter == 'class') and (taxfilter == 'order') and (taxfilter == 'family') and (taxfilter == 'genus'):
    logfile=open(log,"a+")
    logfile.write("\tERROR! The assigned taxonomy filter " + taxfilter + " is incorrect.\n")
    logfile.write("\tThe allowed values include: 'division', 'phylum', 'class', 'order', 'family' and 'genus'.\n")
    logfile.close()
    print("\tERROR! The assigned taxonomy parameter " + taxfilter + " is incorrect")
    print("\tThe allowed values include: 'division', 'phylum', 'class', 'order', 'family' and 'genus.")
    sys.exit(0)

###############################
# Prepare working directories #
###############################

# Set input directory
dir0 = os.path.join(path,"0-Data")
#dir1 = os.path.join(path,"1-Trimmed")
#dir2 = os.path.join(path,"2-Filtered")

## If input directory does not exist, make it
if not os.path.exists(dir0):
    os.makedirs(dir0)

## If stats directory does not exist, make it
statsdir = os.path.join(path,"0-Stats")
if not os.path.exists(statsdir):
    os.makedirs(statsdir)

#################
# Transfer data #
#################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | TRANSFERRING DATA TO THE WORKING DIRECTORY \r\n".format(current_time))
logfile.close()
print("\n{0} | TRANSFERRING DATA TO THE WORKING DIRECTORY".format(current_time))


####
# Add error if input data contains duplicated sequences
####

# Read input data file
inputfile = open(in_f, "r")

#Declare empty output file list
primlist = []
csvlist = []
filtlist = []
dadalist = []

## Read input data row by row
for line in inputfile:
    ### Skip line if starts with # (comment line)
    if not (line.startswith('#')):
        ###Skip line if it's empty
        if not len(line.strip()) == 0 :
            #Define variables
            line=line.replace('\n', '')
            linelist = line.split(',') # Create a list of each line

            #Test whether the list contains 4 elements
            if len(linelist) != 4:
                logfile=open(log,"a+")
                logfile.write("\tThe format of the data input file is incorrect.\n")
                print("\tERROR! The format of the data input file is incorrect.")
                print("\tEnsure the file is comma-separated and contains")
                print("\tfour columns: SAMPLE, POOL, FORWARD, REVERSE. e.g.:")
                print("\tSAMPLE1,POOL1,Sample1_1.fq.gz,Sample1_2.fq.gz")
                print("\tSAMPLE2,POOL1,Sample2_1.fq.gz,Sample2_2.fq.gz\n")
                logfile.close()
                sys.exit(0)

            #Assign variables
            name=linelist[0]
            run=linelist[1]
            in_for=linelist[2]
            in_rev=linelist[3]
            print("\tTransferring sample: "+name)

            #Create run folder if it does not exist
            runpath=path+'/0-Data/'+run
            if not os.path.exists(runpath):
                os.makedirs(runpath)

            #Create list of final files for workflow DADA2 A
            dada_run = path+'/3-DADA2/'+run+'.rds'
            dadalist.append(dada_run)

            # Transfer, rename and decompress data

            #Check if the file is already in the working directory
            out1=runpath+'/'+name+'_1.fastq'
            if os.path.isfile(out1):
                logfile=open(log,"a+")
                logfile.write("\tThe file " + out1 + " is already in the working directory.\n")
                logfile.close()
            else:
                #If the file is not in the working directory, transfer it
                if os.path.isfile(in_for):
                    if in_for.endswith('.gz'):
                        read1Cmd = 'gunzip -c '+in_for+' > '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                    else:
                        read1Cmd = 'cat '+in_for+' > '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_for + " does not exist.\n")
                    logfile.close()

            #Check if the file is already in the working directory
            out2=runpath+'/'+name+'_2.fastq'
            if os.path.isfile(out2):
                logfile=open(log,"a+")
                logfile.write("\tThe file " + out2 + " is already in the working directory.\n")
                logfile.close()
            else:
                #If the file is not in the working directory, transfer it
                if os.path.isfile(in_rev):
                    if in_for.endswith('.gz'):
                        read2Cmd = 'gunzip -c '+in_rev+' > '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                    else:
                        read2Cmd = 'cat '+in_rev+' > '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_rev + " does not exist.\n")
                    logfile.close()

            #Create lists of output files
            prim_for = path+'/1-Primertrimmed/'+run+'/'+name+'_1.fastq'
            prim_rev = path+'/1-Primertrimmed/'+run+'/'+name+'_2.fastq'
            out_csv = path+'/2-Filtered/'+run+'/'+name+'.csv'
            filt_for = path+'/2-Filtered/'+run+'/'+name+'_1.fastq'
            filt_rev = path+'/2-Filtered/'+run+'/'+name+'_2.fastq'

            primlist.append(prim_for)
            primlist.append(prim_rev)
            csvlist.append(out_csv)
            filtlist.append(filt_for)
            filtlist.append(filt_rev)

#####################################################
################## Begin workflows ##################
#####################################################

##############################
# Primer clipping
##############################

if not all(list(map(os.path.isfile,primlist))):
       if (primF == 'noprimer') & (primR == 'noprimer'):
           print("\n{0} | SKIPPING PRIMER CLIPPING".format(current_time))
           print("     As no primer sequence has been declared, BAMSE considers primers have been")
           print("already clipped from the reads. If you want to clip primers from the read")
           print("declare primer sequences in flags -f and -r :")
           print("      e.g. -f CTANGGGNNGCANCAG -r GACTACNNGGGTATCTAAT.")
           # Output log to logfile
           logfile=open(log,"a+")
           current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
           logfile.write("\n{0} | Output files of the preprocessing workflow exist. \r\n".format(current_time))
           logfile.close()

       else:
           print("\n{0} | CLIPPING PRIMER SEQUENCES".format(current_time))
           print("     BAMSE will use Cutadapt to clip the forward and reverse primers from")
           print("the paired end reads. It will automatically detect if all sequences are")
           print("directional (output of PCR-based libraries) or not (output of ligation-based")
           print("libraries), and it will flip the reversed reads in the case of the latter.")
           print("Primer-trimmed files will be stored in the folder '1-Primertrimmed'.\n")
           # Output log to logfile
           logfile=open(log,"a+")
           current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
           logfile.write("\n{0} | Output files of the preprocessing workflow exist. \r\n".format(current_time))
           logfile.close()

           # Define snakefile
           path_snkf = os.path.join(bamsepath,'workflows/primertrimming/Snakefile')
           # Transform output file list into space-separated string (only for development)
           out_primertrim = " ".join(primlist)
           # Run snakemake
           prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_primertrim+' --configfile '+param+' --cores '+cores+' -q'
           subprocess.Popen(prep_snk_Cmd, shell=True).wait()
else:
       # Output log to logfile
       logfile=open(log,"a+")
       current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
       logfile.write("\n{0} | Output files of the preprocessing workflow exist. \r\n".format(current_time))
       logfile.close()
       print("\n{0} | SKIPPING PRIMER CLIPPING".format(current_time))
       print("     The output files of the preprocessing workflow exist in the project")
       print("directory. If you want to re-run this step, remove the files in the directories")
       print("'1-Primertrimmed' and '2-Filtered'.")

##############################
# Calculate optimal trimming scores, filter and trim
##############################

if not all(list(map(os.path.isfile,filtlist))):
    if (primF == 'noprimer') & (primR == 'noprimer'):
        #Calculate optimal trimming scores from primer-clipped files
        print("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES".format(current_time))
        print("     As no primer sequence has been declared, BAMSE will use the raw")
        print("reads for calculating the optimal trimming scores with a minimum overlap")
        print("value of "+str(overlap)+" and maximum expected error value of MEE="+str(mee)+".")
        print("This step can take a while if the size of the number and size of the files is large.\n")
        # Output log to logfile
        logfile=open(log,"a+")
        current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
        logfile.write("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES \r\n".format(current_time))
        logfile.close()

        prepropath=path+'/2-Filtered/'
        if not os.path.exists(prepropath):
            os.makedirs(prepropath)

        # Define snakefile
        path_snkf = os.path.join(bamsepath,'workflows/findscores2/Snakefile')
        #Transform output file list into space-separated string (only for development)
        out_preprocessing = " ".join(csvlist)

        # Run find scores snakemake
        prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_preprocessing+' --configfile '+param+' --cores '+cores+' -q'
        subprocess.Popen(prep_snk_Cmd, shell=True).wait()

        #### Find best score ####
        bastscore_Cmd = 'Rscript '+bamsepath+'/bin/bamse-optimaltrim.R -i '+path+'/2-Filtered/ -o '+path+'/2-Filtered/trim.txt -p '+param+' -l '+log+' 2>> /dev/null'
        subprocess.Popen(bastscore_Cmd, shell=True).wait()

        # Print best score
        trimparameters = []
        trimfile=path+'/2-Filtered/trim.txt'
        with open(trimfile, 'r') as arch:
            for line in arch:
                trimparameters.append(eval(line.rstrip()))
        forwardtrim=trimparameters[0]
        reversetrim=trimparameters[1]

        print("\nThe analysis of optimal trimming parameters concluded that:")
        print("\tThe optimal position to trim Forward reads is at position "+str(forwardtrim)+".")
        print("\tThe optimal position to trim Reverse reads is at position "+str(reversetrim)+".")

        # Filter and trim
        print("\n{0} | TRIMMING AND FILTERING READS".format(current_time))
        print("     BAMSE will trim the forward reads at position "+str(forwardtrim)+" and reverse reads")
        print("at position "+str(reversetrim)+", before calculating expected error values and filtering out")
        print("the reads with MEE values under "+str(mee)+".\n")
        filterandtrim_Cmd = 'Rscript '+bamsepath+'/bin/bamse-filterandtrim.R -i '+path+'/0-Data/ -o '+path+'/2-Filtered/ -t '+path+'/2-Filtered/trim.txt -e '+mee+''
        subprocess.Popen(filterandtrim_Cmd, shell=True).wait()

    else:
        #Calculate optimal trimming scores from original files
        print("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES".format(current_time))
        print("     BAMSE will use primer-clipped reads for calculating the optimal")
        print("trimming scores with a minimum overlap value of "+str(overlap)+" and a maximum")
        print("expected error of MEE="+str(mee)+".\n")
        # Output log to logfile
        logfile=open(log,"a+")
        current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
        logfile.write("\n{0} | CALCULATING OPTIMAL TRIMMING SCORES \r\n".format(current_time))
        logfile.close()

        prepropath=path+'/2-Filtered/'
        if not os.path.exists(prepropath):
            os.makedirs(prepropath)

        path_snkf = os.path.join(bamsepath,'workflows/findscores1/Snakefile')
        #Transform output file list into space-separated string (only for development)
        out_preprocessing = " ".join(csvlist)

        # Run snakemake
        prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_preprocessing+' --configfile '+param+' --cores '+cores+' -q'
        subprocess.Popen(prep_snk_Cmd, shell=True).wait()

        #### Find best score ####
        bastscore_Cmd = 'Rscript '+bamsepath+'/bin/bamse-optimaltrim.R -i '+path+'/2-Filtered/ -o '+path+'/2-Filtered/trim.txt -p '+param+' -l '+log+' 2>> /dev/null'
        subprocess.Popen(bastscore_Cmd, shell=True).wait()

        # Print best score
        trimparameters = []
        trimfile=path+'/2-Filtered/trim.txt'
        with open(trimfile) as f:
            content = f.readlines()
        trimparameters = [x.strip() for x in content]
        forwardtrim=trimparameters[0]
        reversetrim=trimparameters[1]
        method=trimparameters[2]

        if (method == 'average'):
            print("\nWARNING! The analysis of optimal trimming parameters concluded that:")
            print("\tPrimers cannot be trimmed because the read length is too short.")
            print("\tTry shortening the read overlap length or manual trimming position assessment.")
            print("\tBAMSE will continue without trimming reads and only filtering based on MEE values.")

            # Filter and trim
            print("\n{0} | FILTERING READS (WITHOUT TRIMMING)".format(current_time))
            print("     BAMSE will calculatEexpected error values and filter out ")
            print("the reads with MEE values under "+str(mee)+".\n")

        else:
            print("\nThe analysis of optimal trimming parameters concluded that:")
            print("\tThe optimal position to trim Forward reads is at position "+str(forwardtrim)+".")
            print("\tThe optimal position to trim Reverse reads is at position "+str(reversetrim)+".")

            # Filter and trim
            print("\n{0} | TRIMMING AND FILTERING READS".format(current_time))
            print("     BAMSE will trim the forward reads at position "+str(forwardtrim)+" and")
            print("reverse reads at position "+str(reversetrim)+", before calculating expected error values")
            print("and filtering out the reads with MEE values under "+str(mee)+".\n")

        filterandtrim_Cmd = 'Rscript '+bamsepath+'/bin/bamse-filterandtrim.R -i '+path+'/1-Primertrimmed/ -o '+path+'/2-Filtered/ -t '+path+'/2-Filtered/trim.txt -e '+str(mee)+''
        subprocess.Popen(filterandtrim_Cmd, shell=True).wait()

else:
       # Output log to logfile
       print("\n{0} | SKIPPING CALCULATION OF OPTIMAL TRIMMING SCORES".format(current_time))
       print("     The output files of the read trimming and filtering step exist in the project")
       print("directory. If you want to re-run this step, remove the files in the directory")
       print("'2-Filtered'.\n")
       logfile=open(log,"a+")
       current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
       logfile.write("\n{0} | Output files of the trimming and filtering stepexist. \r\n".format(current_time))
       logfile.close()

########################
# Run DADA2 A workflow #     Error learning and dada2 of each run individually
########################

# Define output names
dadalist2 = list(dict.fromkeys(dadalist))
out_dada2a = " ".join(dadalist2)
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2a/Snakefile')

if all(list(map(os.path.isfile,dadalist2))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the DADA2 A workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING DADA2 ERROR LEARNING AND ASV GENERATION WORKFLOW.".format(current_time))
    print("     The output files of the DADA2 error learning and ASV generation workflow")
    print("exist in the project directoy. If you want to re-run this step, remove the files ")
    print("in the directory '3-DADA2'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running DADA2 error learning \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING DADA2 ERROR LEARNING AND ASV GENERATION WORKFLOW.".format(current_time))
    print("     BAMSE will run DADA2 for learning errors from a subset of")
    print("samples and correct sequencing errors from all files before generating ASVs.")
    print("In large datasets this step might take a while.\n")


# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2a+' --configfile '+param+' --cores '+cores+' -q'
subprocess.Popen(prep_snk_Cmd, shell=True).wait()

########################
# Run DADA2 B workflow #      Merge the different runs, and output results
########################

# Define output names
out_dada2b = path+'/ASV_counts.csv '+path+'/ASVs.fasta '+path+'/ASV_taxa.txt'
out_dada2b_list = out_dada2b.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2b/Snakefile')

if all(list(map(os.path.isfile,out_dada2b_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the DADA2 B workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING DADA2 CHIMERA FILTERING AND TAXONOMY ANNOTATION WORKFLOW.".format(current_time))
    print("     The output files of the DADA2 chimera filtering and taxonomy annotation workflow")
    print("exist in the project directoy. If you want to re-run this step, remove the files ")
    print("in the directory '4-Taxonomyfilter', as well as the files 'ASVs.fasta', 'ASV_counts.csv'")
    print("and 'ASV_taxa.txt'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running DADA2 chimera filtering and taxonomy annotation \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING DADA2 CHIMERA FILTERING AND TAXONOMY ANNOTATION WORKFLOW.".format(current_time))
    print("     BAMSE will run DADA2 for filtering chimeric sequences and assigning")
    print("taxonomy to the ASVs. BAMSE will only retain ASVs assigned at least to a")
    print("Bacteria/Archaea Phylum level. In large datasets this step might take a while.\n")

    taxonfilterpath=path+'/4-Taxonomyfilter/'
    if not os.path.exists(taxonfilterpath):
        os.makedirs(taxonfilterpath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2b+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()

#####################
# Run LULU workflow #      Curate ASV list and resulting ASV table
#####################

if luluarg == True:

    lulupath=path+'/5-LULU/'
    if not os.path.exists(taxonfilterpath):
        os.makedirs(taxonfilterpath)

    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running the LULU curation \r\n".format(current_time))
    logfile.close()
    print("\n{0} | RUNNING LULU CURATION.".format(current_time))
    print("     BAMSE will run LULU, because you have specified the flag -u.")
    print("Filtered ASV sequence document and tables will be named as")
    print("ASVs.lulu.fasta and ASV_counts.lulu.csv, respectively.\n")

    # Define output names
    out_lulu = path+'/ASV_counts.lulu.csv '+path+'/ASVs.lulu.fasta'
    curr_dir = os.path.dirname(sys.argv[0])
    bamsepath = os.path.abspath(curr_dir)
    path_snkf = os.path.join(bamsepath,'workflows/lulu/Snakefile')

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_lulu+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()


#########################################
# Common contaminant detection workflow #
#########################################



###############################
# Run ASV clustering workflow #
###############################




#####################
# Run ASV phylogeny #
#####################

# Define output names
out_phylogeny = path+'/6-Phylogeny/ASVs.align.fasta '+path+'/ASVs.tre'
out_phylogeny_list = out_phylogeny.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/phylogeny/Snakefile')

if all(list(map(os.path.isfile,out_phylogeny_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the ASV phylogeny generation workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING ASV PHYLOGENY GENERATION WORKFLOW.".format(current_time))
    print("     The output files of the ASV phylogeny generation workflow exist. If you want to")
    print("re-run this step, remove the files in the directory '6-Phylogeny', as well as the")
    print("file 'ASVs.tre'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running ASV phylogeny \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING ASV PHYLOGENY GENERATION WORKFLOW.".format(current_time))
    print("     BAMSE will first align the ASV sequences to then generate a Maximum")
    print("Likelihood phylogenetic tree. This step might take several hours when the")
    print("number of ASVs is high.\n")

    phylopath=path+'/6-Phylogeny/'
    if not os.path.exists(phylopath):
        os.makedirs(phylopath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_phylogeny+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()

############################
# Run OTU binning workflow #
############################

# Define output names
out_binning = path+'/7-Binning/binmap.txt '+path+'/7-Binning/ASVs.counts.fasta '+path+'/7-Binning/ASVs.sorted.fasta '+path+'/7-Binning/bintable.txt '+path+'/ASVs.binned.fasta '+path+'/ASV_counts.binned.csv '+path+'/ASVs.binned.tre'
out_binning_list = out_binning.split(' ')
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/binning/Snakefile')

if all(list(map(os.path.isfile,out_binning_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Output files of the ASV binning workflow exist. \r\n".format(current_time))
    logfile.close()
    print("\n{0} | SKIPPING ASV BINNING WORKFLOW.".format(current_time))
    print("     The output files of the ASV binning workflow exist. If you want to re-run")
    print("this step, remove the files in the directory '7-Binning', as well as the")
    print("files 'ASVs.binned.fasta', 'ASV_counts.binned.csv' and 'ASVs.binned.tre'.\n")
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("\n{0} | Running ASV binning \r\n".format(current_time))
    logfile.close()
    print("\n{0} | STARTING ASV BINNING WORKFLOW.".format(current_time))
    print("     BAMSE will bin ASVs with sequence identity of >97%. The binned ASV ")
    print("sequence file, ASV count table and ASV phylogenetic tree will be named")
    print("ASVs.binned.fasta, ASV_counts.binned.csv and ASV.binned.tre, respectively.\n")

    binpath=path+'/7-Binning/'
    if not os.path.exists(binpath):
        os.makedirs(binpath)

    # Run snakemake
    prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_binning+' --configfile '+param+' --cores '+cores+' -q'
    subprocess.Popen(prep_snk_Cmd, shell=True).wait()


###################################
# Run diversity analysis workflow #
###################################

###################################
# Merge stats files into one file #
###################################

if all(list(map(os.path.isfile,out_binning_list))):
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("{0} | BAMSE has completed succesfully \r\n".format(current_time))
    logfile.close()
    print("{0} | BAMSE HAS COMPLETED SUCCESFULLY! \r\n".format(current_time))
else:
    # Output log to logfile
    logfile=open(log,"a+")
    current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
    logfile.write("{0} | Not all expected files have been created. Check logfile. \r\n".format(current_time))
    logfile.close()
    print("{0} | SOME EXPECTED FILES HAVE NOT BEEN CREATED!".format(current_time))
    print("Check the logfile for more information.")
