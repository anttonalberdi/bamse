#!/usr/bin/env python

import os
import sys

####################
# Check if conda environment bamse-env is active #
####################

syspath = sys.prefix

if not "bamse-env" in syspath:
    print("#####################")
    print("#### BAMSE v1.0 ##### ERROR!")
    print("#####################")
    print("\nThe conda environment bamse-env is not activated\nType the following code before running bamse:\n\tconda activate bamse-env\n")
    sys.exit(0)

import argparse
import subprocess
import ruamel.yaml
import pathlib
import re
import time
from shutil import which

####################
# Argument parsing #
####################
# add overwrite option -w

parser = argparse.ArgumentParser(description='Runs BAMSE pipeline.')
parser.add_argument('-i', help="Data information file", dest="input", required=True)
parser.add_argument('-d', help="Working directory of the project", dest="workdir", required=True)
parser.add_argument('-f', help="Forward primer sequence", dest="primF", required=True)
parser.add_argument('-r', help="Reverse primer sequence", dest="primR", required=True)
parser.add_argument('-a', help="Expected average amplicon length", dest="ampliconlength", required=True)
parser.add_argument('-x', help="Absolute path to the taxonomy database", dest="tax", required=True)
parser.add_argument('-t', help="Number of threads", dest="threads", required=True)
parser.add_argument('-q', help="Quality filtering stringency. 'loose' (q=20), 'default' (q=25), 'strict' (q=30)", dest="qual", required=False)
parser.add_argument('-m', help="Minimum fold to consider parent ASVs for chimera detection", dest="fold", required=False)
parser.add_argument('-p', help="Absolute path to the parameters file that BAMSE will create", dest="param", required=False)
parser.add_argument('-l', help="Absolute path to the log file that BAMSE will create", dest="log", required=False)
parser.add_argument('-w', help="Overwrite contents in the working directory of the project", action='store_true', dest="overwrite", required=False)
args = parser.parse_args()

# Translate arguments
in_f=args.input
path=args.workdir
primF=args.primF
primR=args.primR
ampliconlength=args.ampliconlength
tax=args.tax
fold=args.fold
cores=args.threads
overwrite=args.overwrite

# Retrieve current directory
file = os.path.dirname(sys.argv[0])
curr_dir = os.path.abspath(file)

# Remove contents in the directory
#if overwrite == True:
    #Add removing commands

# Create workign directory if does not exist
if not os.path.exists(path):
    os.makedirs(path)

#Remove last / to the working directory (if necessary)
path = re.sub('/$','',path)

# Define param file
if not (args.param):
    param=os.path.join(os.path.abspath(path),"bamse.yaml")
else:
    param=args.param

# Define log file
if not (args.log):
    log=os.path.join(path,"bamse.log")
else:
    log=args.log

# Define quality filtering value
if not (args.qual):
    qual='default'
else:
    qual=args.qual

# Define quality filtering value
if not (args.fold):
    fold=1
else:
    fold=args.fold

#Remove param file if exists
if os.path.exists(param):
    os.remove(param)

curr_dir=os.path.dirname(sys.argv[0])
bamsepath=os.path.abspath(curr_dir)

#Calculate maximum and minimum lengths
minlength = int(ampliconlength) - 50
maxlength = int(ampliconlength) + 50

###################
# Create log file #
###################

logfile=open(log,"w+")
logfile.write("#####################\n")
logfile.write("#### BAMSE v1.0 #####\n")
logfile.write("#####################\n")
logfile.write("BAMSE is starting at:\n")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\t{0}\n".format(current_time))
logfile.close()

#############################################
# Append information to the parameters file #
#############################################

#Append information to the parameters file
f = open(str(param), "a")
f.write("#BAMSE core paths\n")
f.write("bamsepath:\n "+str(curr_dir)+"\n")
f.write("projectpath:\n "+str(path)+"\n")
f.write("parampath:\n "+str(param)+"\n")
f.write("logpath:\n "+str(log)+"\n")
f.write("taxonomy:\n "+str(tax)+"\n")
f.write("fold:\n "+str(fold)+"\n")
f.write("\n#Primers\n")
f.write("primer1:\n "+str(primF)+"\n")
f.write("primer2:\n "+str(primR)+"\n")
f.write("\n#Trimming and filtering\n")
f.write("ampliconlength:\n "+str(ampliconlength)+"\n")
f.write("minlength:\n "+str(minlength)+"\n")
f.write("maxlength:\n "+str(maxlength)+"\n")
f.write("qual:\n "+str(qual)+"\n")
f.close()

#Append information to the log file
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\nBAMSE is running with the following parameters:\n")
logfile.write("#Paths\n")
logfile.write("\tbamsepath: "+str(curr_dir)+"\n")
logfile.write("\tprojectpath: "+str(path)+"\n")
logfile.write("\tparampath: "+str(param)+"\n")
logfile.write("\tlogpath: "+str(log)+"\n")
logfile.write("\ttaxonomy: "+str(tax)+"\n")
logfile.write("\tfold: "+str(fold)+"\n")
logfile.write("#Primers\n")
logfile.write("\tprimer1: "+str(primF)+"\n")
logfile.write("\tprimer2: "+str(primR)+"\n")
logfile.write("#Trimming and filtering\n")
logfile.write("\tampliconlength: "+str(ampliconlength)+"\n")
logfile.write("\tminlength: "+str(minlength)+"\n")
logfile.write("\tmaxlength: "+str(maxlength)+"\n")
logfile.write("\t\qual: "+str(qual)+"\n")
logfile.close()

######################
# Check dependencies #
######################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Checking dependencies \r\n".format(current_time))

def is_tool(name):
    return which(name) is not None

if not is_tool('perl'):
    logfile.write("\tPerl is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('snakemake'):
    logfile.write("\Snakemake is not installed or loaded. \n")
    sys.exit(0)

if not is_tool('R'):
    logfile.write("\tR is not installed or loaded. \n")
    sys.exit(0)

else:
    dadacheck='Rscript '+bamsepath+'/bin/dadacheck.R'
    dadacheckresult=subprocess.Popen(dadacheck, shell=True).wait()
    if dadacheckresult == 'FALSE':
        logfile.write("\tDada2 is not installed in R. \n")
        sys.exit(0)

logfile.write("\tAll dependencies are properly installed. \n")
logfile.close()

#########################################
# Check if input information is correct #
#########################################

if not os.path.isfile(in_f):
    logfile.write("\tThe input data file does not exist. \n")
    sys.exit(0)

if not os.path.isfile(tax):
    logfile.write("\tThe taxonomy database path is incorrect. \n")
    sys.exit(0)

if not (qual == 'loose') or (qual == 'default') or (qual == 'strict'):
    logfile=open(log,"a+")
    logfile.write("\tThe assigned quality parameter " + qual + " is incorrect.\n")
    logfile.close()

###############################
# Prepare working directories #
###############################

# Set input directory
dir0 = os.path.join(path,"0-Data")
#dir1 = os.path.join(path,"1-Trimmed")
#dir2 = os.path.join(path,"2-Filtered")

## If input directory does not exist, make it
if not os.path.exists(dir0):
    os.makedirs(dir0)

## If stats directory does not exist, make it
statsdir = os.path.join(path,"0-Stats")
if not os.path.exists(statsdir):
    os.makedirs(statsdir)

#################
# Transfer data #
#################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Transferring data to the working directory \r\n".format(current_time))
logfile.close()

####
# Add error if input data contains duplicated sequences
####

# Read input data file
inputfile = open(in_f, "r")

#Declare empty output file list
outlist = []
runlist = []

## Read input data row by row
for line in inputfile:
    ### Skip line if starts with # (comment line)
    if not (line.startswith('#')):
        ###Skip line if it's empty
        if not len(line.strip()) == 0 :
            #Define variables
            line=line.replace('\n', '')
            linelist = line.split(',') # Create a list of each line
            name=linelist[0]
            sample=linelist[1]
            run=linelist[2]
            in_for=linelist[3]
            in_rev=linelist[4]

            #Create run folder if it does not exist
            runpath=path+'/0-Data/'+run
            if not os.path.exists(runpath):
                os.makedirs(runpath)

            #Create list of final files for workflow DADA2 A
            out_run = path+'/3-DADA2/'+run+'.rds'
            runlist.append(out_run)

            # Transfer, rename and decompress data

            #Check if the file is already in the working directory
            out1=runpath+'/'+name+'_1.fastq'
            if os.path.isfile(out1):
                logfile=open(log,"a+")
                logfile.write("\tThe file " + out1 + " is already in the working directory.\n")
                logfile.close()
            else:
                #If the file is not in the working directory, transfer it
                if os.path.isfile(in_for):
                    if in_for.endswith('.gz'):
                        read1Cmd = 'gunzip -c '+in_for+' > '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                    else:
                        read1Cmd = 'cp '+in_for+' '+runpath+'/'+name+'_1.fastq'
                        subprocess.Popen(read1Cmd, shell=True).wait()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_for + " does not exist.\n")
                    logfile.close()

            #Check if the file is already in the working directory
            out2=runpath+'/'+name+'_2.fastq'
            if os.path.isfile(out2):
                logfile=open(log,"a+")
                logfile.write("\tThe file " + out2 + " is already in the working directory.\n")
                logfile.close()
            else:
                #If the file is not in the working directory, transfer it
                if os.path.isfile(in_rev):
                    if in_for.endswith('.gz'):
                        read2Cmd = 'gunzip -c '+in_rev+' > '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                    else:
                        read2Cmd = 'cp '+in_rev+' '+runpath+'/'+name+'_2.fastq'
                        subprocess.Popen(read2Cmd, shell=True).wait()
                else:
                    logfile=open(log,"a+")
                    logfile.write("\tThe file " + in_rev + " does not exist.\n")
                    logfile.close()

            #Create list of output files for preprocessing workflow
            out_for = path+'/2-Filtered/'+run+'/'+name+'_1.fastq'
            out_rev = path+'/2-Filtered/'+run+'/'+name+'_2.fastq'
            outlist.append(out_for)
            outlist.append(out_rev)

#####################################################
################## Begin workflows ##################
#####################################################

##############################
# Run preprocessing workflow #     Prepare data for DADA2 pipeline (trimming, quality filtering, etc.)
##############################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Starting preprocessing workflow \r\n".format(current_time))
logfile.close()

path_snkf = os.path.join(bamsepath,'workflows/preprocessing2/Snakefile')
#Transform output file list into space-separated string (only for development)
out_preprocessing = " ".join(outlist)

# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_preprocessing+' --configfile '+param+' --cores '+cores+''
subprocess.Popen(prep_snk_Cmd, shell=True).wait()

########################
# Run DADA2 A workflow #     Error learning and dada2 of each run individually
########################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Starting dada2 workflow part A \r\n".format(current_time))
logfile.close()

# Define output names
runlist2 = list(dict.fromkeys(runlist))
out_dada2a = " ".join(runlist2)
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2a/Snakefile')

# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2a+' --configfile '+param+' --cores '+cores+''
subprocess.Popen(prep_snk_Cmd, shell=True).wait()

########################
# Run DADA2 B workflow #      Merge the different runs, and output results
########################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Starting dada2 workflow part B \r\n".format(current_time))
logfile.close()

# Define output names
out_dada2b = path+'/ASV_counts.txt '+path+'/ASVs.fasta '+path+'/ASV_taxa.txt'
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/dada2b/Snakefile')

# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_dada2b+' --configfile '+param+' --cores '+cores+''
subprocess.Popen(prep_snk_Cmd, shell=True).wait()

#####################
# Run LULU workflow #      Curate ASV list and resulting ASV table
#####################

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Running the polishing workflow \r\n".format(current_time))
logfile.close()

# Define output names
out_polishing = path+'/ASV_counts_lulu.txt '+path+'/ASVs_lulu.fasta'
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/polishing/Snakefile')

# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_polishing+' --configfile '+param+' --cores '+cores+''
subprocess.Popen(prep_snk_Cmd, shell=True).wait()

#########################################
# Common contaminant detection workflow #
#########################################



###############################
# Run ASV clustering workflow #
###############################




#####################
# Run ASV phylogeny #
#####################

phylopath=path+'/4-Phylogeny/'
if not os.path.exists(phylopath):
    os.makedirs(phylopath)

# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("\n{0} | Running the polishing workflow \r\n".format(current_time))
logfile.close()

# Define output names
out_polishing = path+'/4-Phylogeny/ASVs.align.fasta '+path+'/ASVs.tre'
curr_dir = os.path.dirname(sys.argv[0])
bamsepath = os.path.abspath(curr_dir)
path_snkf = os.path.join(bamsepath,'workflows/phylogeny/Snakefile')

# Run snakemake
prep_snk_Cmd = 'snakemake -s '+path_snkf+' -k '+out_polishing+' --configfile '+param+' --cores '+cores+''
subprocess.Popen(prep_snk_Cmd, shell=True).wait()



###################################
# Run diversity analysis workflow #
###################################

###################################
# Merge stats files into one file #
###################################





# Output log to logfile
logfile=open(log,"a+")
current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
logfile.write("{0} | BAMSE has completed succesfully \r\n".format(current_time))
logfile.close()
